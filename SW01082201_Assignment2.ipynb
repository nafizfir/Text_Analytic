{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea5f3b77-307f-4ee0-a786-56dd83ceabca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.57.0-cp312-cp312-win_amd64.whl.metadata (104 kB)\n",
      "     ---------------------------------------- 0.0/104.6 kB ? eta -:--:--\n",
      "     ---------------------- ---------------- 61.4/104.6 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 104.6/104.6 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nafiz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 174.1/294.9 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.9/294.9 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 16.2 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.7/8.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.4/8.1 MB 11.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.6/8.1 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.4/8.1 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.8/8.1 MB 9.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.1/8.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.5/8.1 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.9/8.1 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.3/8.1 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.9/8.1 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.3/8.1 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.6/8.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.9/8.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/8.1 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.0/8.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.1/111.1 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, matplotlib, seaborn\n",
      "Successfully installed cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.3 seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\nafiz\\anaconda3\\Lib\\site-packages\\scipy-1.15.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\nafiz\\anaconda3\\Lib\\site-packages\\scipy-1.15.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\nafiz\\anaconda3\\Lib\\site-packages\\scipy-1.15.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\nafiz\\anaconda3\\Lib\\site-packages\\scipy-1.15.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\nafiz\\anaconda3\\Lib\\site-packages\\scipy-1.15.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d079ca3-901f-40f5-b2ff-de2a76a4cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ef4f830-bc2d-4bb9-abdd-003f698004c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "df = df[['Score', 'Summary', 'Text']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da71cb12-1e0e-4573-b5c9-65e2cd28ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine summary and review text\n",
    "df['review'] = df['Summary'] + \" \" + df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "826a27d6-c206-4913-9a38-0767a8b8abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Score to sentiment (1-2: negative, 3: neutral, 4-5: positive)\n",
    "def get_sentiment(score):\n",
    "    if score <= 2:\n",
    "        return 'negative'\n",
    "    elif score == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "df['sentiment'] = df['Score'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe723ef0-dbaa-4f05-8f2e-1f9c497fd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for faster training\n",
    "df_sampled = df.sample(10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "becd933f-c1e9-479d-ab5f-05157b486710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf9b854b-0080-40d0-94f4-fdec3e4253b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nafiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nafiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nafiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8c92e4e-f5ab-4e1c-8af0-aab6440662e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1ddec30-2d20-4bc2-95d8-790df9713ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df_sampled['clean_review'] = df_sampled['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cea36e3-a0ae-4262-a060-75af9a0a03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Feature Extraction \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df_sampled['clean_review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "174b8ee8-da91-45a4-96c6-0fee7aa39446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_sampled['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35642875-1dce-4172-858c-8c5a3a6fa68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "586a03ca-9bcb-4983-9742-54b1da50bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': LinearSVC()\n",
    "}\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85fa53d7-bcc9-4ea4-900f-f4f5c09a5757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes - Accuracy: 0.7980\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.08      0.15       303\n",
      "     neutral       0.00      0.00      0.00       125\n",
      "    positive       0.80      1.00      0.89      1572\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.59      0.36      0.34      2000\n",
      "weighted avg       0.77      0.80      0.72      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafiz\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nafiz\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nafiz\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression - Accuracy: 0.8430\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.45      0.57       303\n",
      "     neutral       0.23      0.02      0.04       125\n",
      "    positive       0.85      0.98      0.91      1572\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.62      0.49      0.51      2000\n",
      "weighted avg       0.80      0.84      0.81      2000\n",
      "\n",
      "\n",
      "SVM - Accuracy: 0.8540\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.59      0.65       303\n",
      "     neutral       0.30      0.11      0.16       125\n",
      "    positive       0.89      0.96      0.92      1572\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.64      0.56      0.58      2000\n",
      "weighted avg       0.83      0.85      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} - Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    results[name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ac06a26-13de-444c-9a7a-aed53290e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nafiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 4. Lexicon-Based Approach using VADER \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e581760-68b4-4faa-a098-14be10751904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VADER Lexicon-Based Accuracy: 0.8175\n"
     ]
    }
   ],
   "source": [
    "df_sampled['vader_sentiment'] = df_sampled['review'].apply(get_vader_sentiment)\n",
    "vader_accuracy = accuracy_score(df_sampled['sentiment'], df_sampled['vader_sentiment'])\n",
    "print(f\"\\nVADER Lexicon-Based Accuracy: {vader_accuracy:.4f}\")\n",
    "results['VADER'] = vader_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afa2e088-8cf6-49e3-9241-9cafe7a24638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discussion: \n",
      "Among the models tested, Support Vector Machine (SVM) and Logistic Regression performed the best in terms of accuracy. \n",
      "Naive Bayes was slightly behind but still decent. Lexicon-based approach using VADER provided fast results but \n",
      "struggled with neutral sentiment and longer reviews due to lack of context understanding.\n",
      "\n",
      "Strengths:\n",
      "- SVM and Logistic Regression are robust for high-dimensional text data like TF-IDF.\n",
      "- VADER is fast and interpretable, ideal for quick sentiment snapshots.\n",
      "\n",
      "Weaknesses:\n",
      "- Lexicon-based models cannot adapt to new data patterns or context.\n",
      "- ML models require training data, computational power, and tuning for best results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Discussion Section\n",
    "discussion = \"\"\"\n",
    "Discussion: \n",
    "Among the models tested, Support Vector Machine (SVM) and Logistic Regression performed the best in terms of accuracy. \n",
    "Naive Bayes was slightly behind but still decent. Lexicon-based approach using VADER provided fast results but \n",
    "struggled with neutral sentiment and longer reviews due to lack of context understanding.\n",
    "\n",
    "Strengths:\n",
    "- SVM and Logistic Regression are robust for high-dimensional text data like TF-IDF.\n",
    "- VADER is fast and interpretable, ideal for quick sentiment snapshots.\n",
    "\n",
    "Weaknesses:\n",
    "- Lexicon-based models cannot adapt to new data patterns or context.\n",
    "- ML models require training data, computational power, and tuning for best results.\n",
    "\"\"\"\n",
    "\n",
    "print(discussion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
